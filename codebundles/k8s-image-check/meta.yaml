commands:
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o json | jq -r ''[.items[] | "Images: " + (.spec.containers[].image|tostring)
    + ", Last Started Times:" + (.status.containerStatuses[].state.running.startedAt|tostring)]'''
  doc_links: '

    - [kubectl Overview](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/){:target="_blank"}

    - [kubectl get](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get){:target="_blank"}

    - [jq Manual](https://stedolan.github.io/jq/manual/){:target="_blank"}'
  explanation: This command uses kubectl to retrieve information about running pods
    within a specific context and namespace, filtering for those in the "Running"
    state. It then formats the output in JSON using jq to display each pod's container
    image and last start time.
  multi_line_details: '

    # This command retrieves information about running pods in a specific context
    and namespace, then formats the output using jq to display container images and
    their last start times.


    # Set the context and namespace variables for the kubectl command

    CONTEXT=my-context

    NAMESPACE=my-namespace


    # Get the pods in the specified context and namespace that are in the Running
    state and output the results as JSON

    kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o json | \


    # Use jq to format the JSON output into a readable format including container
    images and their last start times

    jq -r ''[.items[] | "Images: " + (.spec.containers[].image|tostring) + ", Last
    Started Times:" + \

    (.status.containerStatuses[].state.running.startedAt|tostring)]''

    '
  name: check_image_rollover_times_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: DevOps
    or Site Reliability Engineers may perform this task to analyze the container images
    and start times of pods experiencing CrashLoopBackoff events in order to identify
    and resolve the underlying issues.


    2. Monitoring for abnormal pod behavior: DevOps or Site Reliability Engineers
    may use this command to regularly check for any abnormal pod behavior, such as
    excessive restarts or long-running pods, in order to proactively address potential
    issues before they impact performance.


    3. Investigating performance issues: When troubleshooting performance issues within
    a specific context and namespace, DevOps or Site Reliability Engineers may use
    this command to gather information on running pods and their container images
    and start times to identify any patterns or outliers that could be impacting performance.


    4. Conducting security audits: DevOps or Site Reliability Engineers may use this
    command as part of regular security audits to ensure that only authorized container
    images are running within a specific context and namespace, and to verify the
    start times of pods to detect any unauthorized or suspicious activity.


    5. Optimizing resource allocation: DevOps or Site Reliability Engineers may use
    this command to gather insights into the container images and start times of running
    pods in order to optimize resource allocation and ensure efficient utilization
    of resources within the Kubernetes environment.'
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o json | jq -r ''[.items[] | "Images: " + (.spec.containers[].image|tostring)
    + ", Last Started Times:" + (.status.containerStatuses[].state.running.startedAt|tostring)]'''
  doc_links: '

    - [kubectl Overview](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/){:target="_blank"}

    - [kubectl get](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get){:target="_blank"}

    - [jq Manual](https://stedolan.github.io/jq/manual/){:target="_blank"}'
  explanation: This command uses kubectl to retrieve information about running pods
    within a specific context and namespace, filtering for those in the "Running"
    state. It then formats the output in JSON using jq to display each pod's container
    image and last start time.
  multi_line_details: '

    # This command retrieves information about running pods in a specific context
    and namespace, then formats the output using jq to display container images and
    their last start times.


    # Set the context and namespace variables for the kubectl command

    CONTEXT=my-context

    NAMESPACE=my-namespace


    # Get the pods in the specified context and namespace that are in the Running
    state and output the results as JSON

    kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o json | \


    # Use jq to format the JSON output into a readable format including container
    images and their last start times

    jq -r ''[.items[] | "Images: " + (.spec.containers[].image|tostring) + ", Last
    Started Times:" + \

    (.status.containerStatuses[].state.running.startedAt|tostring)]''

    '
  name: check_image_rollover_times_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: DevOps
    or Site Reliability Engineers may perform this task to analyze the container images
    and start times of pods experiencing CrashLoopBackoff events in order to identify
    and resolve the underlying issues.


    2. Monitoring for abnormal pod behavior: DevOps or Site Reliability Engineers
    may use this command to regularly check for any abnormal pod behavior, such as
    excessive restarts or long-running pods, in order to proactively address potential
    issues before they impact performance.


    3. Investigating performance issues: When troubleshooting performance issues within
    a specific context and namespace, DevOps or Site Reliability Engineers may use
    this command to gather information on running pods and their container images
    and start times to identify any patterns or outliers that could be impacting performance.


    4. Conducting security audits: DevOps or Site Reliability Engineers may use this
    command as part of regular security audits to ensure that only authorized container
    images are running within a specific context and namespace, and to verify the
    start times of pods to detect any unauthorized or suspicious activity.


    5. Optimizing resource allocation: DevOps or Site Reliability Engineers may use
    this command to gather insights into the container images and start times of running
    pods in order to optimize resource allocation and ensure efficient utilization
    of resources within the Kubernetes environment.'
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o=json | jq -r ''.items[] | "---", "pod_name: " + .metadata.name, "Status: "
    + .status.phase, "containers:", (.spec.containers[] | "- container_name: " + .name,
    " \ image_path: " + (.image | split(":")[0]), " \ image_tag: " + (.image | split(":")[1])),
    "---"'''
  doc_links: '

    - [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [JSON output format in kubectl](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#output-format){:target="_blank"}

    - [jq documentation](https://stedolan.github.io/jq/manual/){:target="_blank"}'
  explanation: This command uses kubectl to retrieve information about running pods
    in a specific context and namespace, then formats the output using JSON and jq
    to display the pod name, status, and container details.
  multi_line_details: "\n# Set the kubectl context to the value of the environment\
    \ variable ${CONTEXT}\n# Set the namespace for the kubectl command using the value\
    \ of the environment variable ${NAMESPACE}\nkubectl get pods --context=${CONTEXT}\
    \ -n ${NAMESPACE} \\\n  --field-selector=status.phase==Running -o=json | \\\n\
    \  # Use jq to parse the JSON output and format it in a human-readable way\n \
    \ jq -r '.items[] | \n    \"---\", \n    \"pod_name: \" + .metadata.name, \n \
    \   \"Status: \" + .status.phase, \n    \"containers:\", \n    (.spec.containers[]\
    \ | \n      \"- container_name: \" + .name, \n     \" \\ image_path: \" + (.image\
    \ | split(\":\")[0]), \n     \" \\ image_tag: \" + (.image | split(\":\")[1])\n\
    \    ), \n   \"---\"'\n\nIn this multi-line command, we've added comments to explain\
    \ each part of the command for those who may be new or less experienced with using\
    \ kubectl and jq. This should make it easier for them to understand what the command\
    \ is doing."
  name: list_images_and_tags_for_every_container_in_running_pods_for_namespace_namespace
  when_is_it_useful: '1. Investigating a service outage: When there is a service outage
    in Kubernetes, a DevOps or Site Reliability Engineer may need to quickly gather
    details about the status of running pods to identify any crashed or failing containers.


    2. Debugging application errors: If an application deployed on Kubernetes is experiencing
    errors or failures, the engineer may use this command to gather detailed information
    about the pods and containers to help diagnose the issue.


    3. Monitoring resource utilization: In order to monitor resource utilization and
    performance of pods and containers, the engineer may regularly use this command
    to retrieve and analyze the relevant information.


    4. Troubleshooting CrashLoopBackoff events: A common issue in Kubernetes, CrashLoopBackoff
    events occur when a container repeatedly crashes and restarts. The engineer may
    use this command to investigate and troubleshoot such events.


    5. Performing maintenance tasks: During routine maintenance or upgrades, the engineer
    may use this command to ensure that all pods and containers are running as expected
    and are not encountering any issues.'
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Running
    -o=json | jq -r ''.items[] | "---", "pod_name: " + .metadata.name, "Status: "
    + .status.phase, "containers:", (.spec.containers[] | "- container_name: " + .name,
    " \ image_path: " + (.image | split(":")[0]), " \ image_tag: " + (.image | split(":")[1])),
    "---"'''
  doc_links: '

    - [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [JSON output format in kubectl](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#output-format){:target="_blank"}

    - [jq documentation](https://stedolan.github.io/jq/manual/){:target="_blank"}'
  explanation: This command uses kubectl to retrieve information about running pods
    in a specific context and namespace, then formats the output using JSON and jq
    to display the pod name, status, and container details.
  multi_line_details: "\n# Set the kubectl context to the value of the environment\
    \ variable ${CONTEXT}\n# Set the namespace for the kubectl command using the value\
    \ of the environment variable ${NAMESPACE}\nkubectl get pods --context=${CONTEXT}\
    \ -n ${NAMESPACE} \\\n  --field-selector=status.phase==Running -o=json | \\\n\
    \  # Use jq to parse the JSON output and format it in a human-readable way\n \
    \ jq -r '.items[] | \n    \"---\", \n    \"pod_name: \" + .metadata.name, \n \
    \   \"Status: \" + .status.phase, \n    \"containers:\", \n    (.spec.containers[]\
    \ | \n      \"- container_name: \" + .name, \n     \" \\ image_path: \" + (.image\
    \ | split(\":\")[0]), \n     \" \\ image_tag: \" + (.image | split(\":\")[1])\n\
    \    ), \n   \"---\"'\n\nIn this multi-line command, we've added comments to explain\
    \ each part of the command for those who may be new or less experienced with using\
    \ kubectl and jq. This should make it easier for them to understand what the command\
    \ is doing."
  name: list_images_and_tags_for_every_container_in_running_pods_for_namespace_namespace
  when_is_it_useful: '1. Investigating a service outage: When there is a service outage
    in Kubernetes, a DevOps or Site Reliability Engineer may need to quickly gather
    details about the status of running pods to identify any crashed or failing containers.


    2. Debugging application errors: If an application deployed on Kubernetes is experiencing
    errors or failures, the engineer may use this command to gather detailed information
    about the pods and containers to help diagnose the issue.


    3. Monitoring resource utilization: In order to monitor resource utilization and
    performance of pods and containers, the engineer may regularly use this command
    to retrieve and analyze the relevant information.


    4. Troubleshooting CrashLoopBackoff events: A common issue in Kubernetes, CrashLoopBackoff
    events occur when a container repeatedly crashes and restarts. The engineer may
    use this command to investigate and troubleshoot such events.


    5. Performing maintenance tasks: During routine maintenance or upgrades, the engineer
    may use this command to ensure that all pods and containers are running as expected
    and are not encountering any issues.'
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Failed
    -o=json | jq -r ''.items[] | "---", "pod_name: " + .metadata.name, "Status: "
    + .status.phase, "containers:", (.spec.containers[] | "- container_name: " + .name,
    " \ image_path: " + \(.image | split(":")[0]), " \ image_tag: " + (.image | split(":")[1])),
    "---"'''
  doc_links: '

    - [Retrieve information about pods in a specific namespace](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get){:target="_blank"}

    - [Filter pods that have failed status](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase){:target="_blank"}

    - [Format and output information in JSON format using jq](https://stedolan.github.io/jq/){:target="_blank"}'
  explanation: This command retrieves information about specific pods in a particular
    namespace that have failed, and then formats and outputs the information in JSON
    format using the jq tool.
  multi_line_details: "\n# Get pods from a specific context and namespace that have\
    \ failed\nkubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Failed\
    \ -o=json \\\n  # Use jq to parse the JSON output and format it for easy reading\n\
    \  | jq -r '.items[] | \"---\", \"pod_name: \" + .metadata.name, \"Status: \"\
    \ + .status.phase, \"containers:\", (.spec.containers[] | \n  \"- container_name:\
    \ \" + .name, \n  \" \\ image_path: \" + \\(.image | split(\":\")[0]), \n  \"\
    \ \\ image_tag: \" + (.image | split(\":\")[1])), \"---\"'\n\n\nThis multi-line\
    \ command breaks down each step of the original command with helpful comments\
    \ for better understanding. It also maintains the same functionality while making\
    \ it easier to read and comprehend for new or less experienced devops engineers."
  name: list_images_and_tags_for_every_container_in_failed_pods_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: The DevOps
    or SRE may need to use this command to gather information about specific pods
    experiencing the CrashLoopBackoff state in order to identify the root cause and
    potential resolution.


    2. Monitoring and investigating frequent pod failures: In a production environment,
    frequent pod failures can be detrimental to the overall health and stability of
    the cluster. Using this command, the DevOps or SRE can retrieve detailed information
    about failing pods to analyze and take corrective actions.


    3. Investigating performance issues: If there are performance degradation or resource
    constraints within the Kubernetes cluster, the DevOps or SRE may need to use this
    command to gather detailed information about failing pods and their resource utilization.


    4. Identifying misbehaving applications: Certain applications or services running
    within the Kubernetes cluster may exhibit abnormal behavior, such as excessive
    restarting or crashing. This command can help the DevOps or SRE to pinpoint the
    problematic pods and investigate the cause of their failure.


    5. Troubleshooting application errors: When end users report errors or malfunctions
    with specific applications or services within the Kubernetes environment, the
    DevOps or SRE may use this command to retrieve detailed information about the
    relevant pods and diagnose potential issues.'
- command: 'kubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Failed
    -o=json | jq -r ''.items[] | "---", "pod_name: " + .metadata.name, "Status: "
    + .status.phase, "containers:", (.spec.containers[] | "- container_name: " + .name,
    " \ image_path: " + \(.image | split(":")[0]), " \ image_tag: " + (.image | split(":")[1])),
    "---"'''
  doc_links: '

    - [Retrieve information about pods in a specific namespace](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get){:target="_blank"}

    - [Filter pods that have failed status](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase){:target="_blank"}

    - [Format and output information in JSON format using jq](https://stedolan.github.io/jq/){:target="_blank"}'
  explanation: This command retrieves information about specific pods in a particular
    namespace that have failed, and then formats and outputs the information in JSON
    format using the jq tool.
  multi_line_details: "\n# Get pods from a specific context and namespace that have\
    \ failed\nkubectl get pods --context=${CONTEXT} -n ${NAMESPACE} --field-selector=status.phase==Failed\
    \ -o=json \\\n  # Use jq to parse the JSON output and format it for easy reading\n\
    \  | jq -r '.items[] | \"---\", \"pod_name: \" + .metadata.name, \"Status: \"\
    \ + .status.phase, \"containers:\", (.spec.containers[] | \n  \"- container_name:\
    \ \" + .name, \n  \" \\ image_path: \" + \\(.image | split(\":\")[0]), \n  \"\
    \ \\ image_tag: \" + (.image | split(\":\")[1])), \"---\"'\n\n\nThis multi-line\
    \ command breaks down each step of the original command with helpful comments\
    \ for better understanding. It also maintains the same functionality while making\
    \ it easier to read and comprehend for new or less experienced devops engineers."
  name: list_images_and_tags_for_every_container_in_failed_pods_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: The DevOps
    or SRE may need to use this command to gather information about specific pods
    experiencing the CrashLoopBackoff state in order to identify the root cause and
    potential resolution.


    2. Monitoring and investigating frequent pod failures: In a production environment,
    frequent pod failures can be detrimental to the overall health and stability of
    the cluster. Using this command, the DevOps or SRE can retrieve detailed information
    about failing pods to analyze and take corrective actions.


    3. Investigating performance issues: If there are performance degradation or resource
    constraints within the Kubernetes cluster, the DevOps or SRE may need to use this
    command to gather detailed information about failing pods and their resource utilization.


    4. Identifying misbehaving applications: Certain applications or services running
    within the Kubernetes cluster may exhibit abnormal behavior, such as excessive
    restarting or crashing. This command can help the DevOps or SRE to pinpoint the
    problematic pods and investigate the cause of their failure.


    5. Troubleshooting application errors: When end users report errors or malfunctions
    with specific applications or services within the Kubernetes environment, the
    DevOps or SRE may use this command to retrieve detailed information about the
    relevant pods and diagnose potential issues.'
- command: 'NAMESPACE=${NAMESPACE}; POD_NAME="skopeo-pod"; CONTEXT="${CONTEXT}"; events=$(kubectl
    get events -n $NAMESPACE --context=$CONTEXT -o json | jq --arg timestamp "$(date
    -u -v -5M +"%Y-%m-%dT%H:%M:%SZ" 2>/dev/null || date -u -d "-5 minutes" +"%Y-%m-%dT%H:%M:%SZ")"
    ''.items[] | select(.lastTimestamp > $timestamp)''); if [[ ! -z "\${events-unset}"
    ]]; then image_pull_backoff_events=$(echo "$events" | jq -s ''[.[] | select(.reason
    == "BackOff") | .message] | .[]''); else echo "No events found in the last 5 minutes";
    exit; fi; if [[ $image_pull_backoff_events =~ "Back-off pulling image" ]]; then
    echo "Running Skopeo Pod"; kubectl run $POD_NAME --restart=Never -n $NAMESPACE
    --context=$CONTEXT --image=quay.io/containers/skopeo:latest --command -- sleep
    infinity && echo "Waiting for the $POD_NAME to be running..." && kubectl wait
    --for=condition=Ready pod/$POD_NAME -n $NAMESPACE --context=$CONTEXT; else echo
    "No image pull backoff events found"; exit; fi; while IFS= read -r event; do echo
    "Found BackOff with message: $event"; echo "Checking if we can reach the image
    with skopeo and what tags exist"; container_image_path_tag=$(echo "$event" | cut
    -d'' '' -f4 | tr -d ''"'' | tr -d ''\''); container_image_path="\${container_image_path_tag%:*}";
    container_image_tag="\${container_image_path_tag#*:}"; if [ -z "$container_image_path"
    ] || [ -z "$container_image_tag" ]; then continue; fi; skopeo_output=$(kubectl
    exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo inspect docker://$container_image_path:$container_image_tag);
    skopeo_exit_code=$?; if [ $skopeo_exit_code -eq 0 ]; then echo "Container image
    ''$container_image_path:$container_image_tag'' exists."; else echo "Container
    image ''$container_image_path:$container_image_tag'' does not exist."; echo "Available
    tags for ''$container_image_path'':"; available_tags=$(kubectl exec $POD_NAME
    -n $NAMESPACE --context=$CONTEXT -- skopeo list-tags docker://$container_image_path
    ); echo "$available_tags"; fi; done <<<"$image_pull_backoff_events" && echo "Deleting
    Skopeo pod" && kubectl delete pod $POD_NAME -n $NAMESPACE --context=$CONTEXT &&
    echo "Done"'
  doc_links: '

    - [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [namespace and context in kubernetes](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#context){:target="_blank"}

    - [Shell scripting documentation](https://www.tutorialspoint.com/unix/unix-shell-functions.htm){:target="_blank"}

    - [Skopeo tool documentation](https://github.com/containers/skopeo){:target="_blank"}'
  explanation: This command is a shell script that uses kubectl to gather information
    about recent events in a specific namespace and context. It then evaluates whether
    there have been any "BackOff" events related to pulling container images, and
    if so, it deploys a new pod using the Skopeo tool to inspect the problematic container
    image and its available tags. If the image exists, it prints a message, otherwise
    it lists available tags and finally deletes the Skopeo pod.
  multi_line_details: "\n# Set the NAMESPACE variable to the environment variable\
    \ of the same name\nNAMESPACE=${NAMESPACE}\n\n# Set the POD_NAME variable to \"\
    skopeo-pod\"\nPOD_NAME=\"skopeo-pod\"\n\n# Set the CONTEXT variable to the environment\
    \ variable of the same name\nCONTEXT=\"${CONTEXT}\"\n\n# Get the events from the\
    \ Kubernetes cluster and filter them based on timestamp\nevents=$(kubectl get\
    \ events -n $NAMESPACE --context=$CONTEXT -o json | jq --arg timestamp \"$(date\
    \ -u -v -5M +\"%Y-%m-%dT%H:%M:%SZ\" 2>/dev/null || date -u -d \"-5 minutes\" +\"\
    %Y-%m-%dT%H:%M:%SZ\")\" '.items[] | select(.lastTimestamp > $timestamp)')\n\n\
    # If there are no events available, exit with a message\nif [[ ! -z \"${events-unset}\"\
    \ ]]; then\n  image_pull_backoff_events=$(echo \"$events\" | jq -s '[.[] | select(.reason\
    \ == \"BackOff\") | .message] | .[]')\nelse\n  echo \"No events found in the last\
    \ 5 minutes\"\n  exit\nfi\n\n# Check if the image pull backoff event contains\
    \ the specific message\nif [[ $image_pull_backoff_events =~ \"Back-off pulling\
    \ image\" ]]; then\n  # Run Skopeo Pod with the specified configuration\n  echo\
    \ \"Running Skopeo Pod\"\n  kubectl run $POD_NAME --restart=Never -n $NAMESPACE\
    \ --context=$CONTEXT --image=quay.io/containers/skopeo:latest --command -- sleep\
    \ infinity && echo \"Waiting for the $POD_NAME to be running...\" && kubectl wait\
    \ --for=condition=Ready pod/$POD_NAME -n $NAMESPACE --context=$CONTEXT\nelse\n\
    \  echo \"No image pull backoff events found\"\n  exit\nfi\n\n# Process each image\
    \ pull backoff event by checking for its availability and retrieving available\
    \ tags using Skopeo\nwhile IFS= read -r event; do\n  echo \"Found BackOff with\
    \ message: $event\"\n  echo \"Checking if we can reach the image with skopeo and\
    \ what tags exist\"\n  container_image_path_tag=$(echo \"$event\" | cut -d' '\
    \ -f4 | tr -d '\"' | tr -d '\\')\n  container_image_path=\"${container_image_path_tag%:*}\"\
    \n  container_image_tag=\"${container_image_path_tag#*:}\"\n  if [ -z \"$container_image_path\"\
    \ ] || [ -z \"$container_image_tag\" ]; then\n    continue\n  fi\n  skopeo_output=$(kubectl\
    \ exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo inspect docker://$container_image_path:$container_image_tag)\n\
    \  skopeo_exit_code=$?\n  if [ $skopeo_exit_code -eq 0 ]; then\n    echo \"Container\
    \ image '$container_image_path:$container_image_tag' exists.\"\n  else\n    echo\
    \ \"Container image '$container_image_path:$container_image_tag' does not exist.\"\
    \n    echo \"Available tags for '$container_image_path':\"\n    available_tags=$(kubectl\
    \ exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo list-tags docker://$container_image_path\
    \ )\n    echo \"$available_tags\"\n  fi\ndone <<<\"$image_pull_backoff_events\"\
    \n\n# Delete the Skopeo pod after processing all events\necho \"Deleting Skopeo\
    \ pod\"\nkubectl delete pod $POD_NAME -n $NAMESPACE --context=$CONTEXT\necho \"\
    Done\"\n"
  name: list_imagepullbackoff_events_and_test_path_and_tags_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: The DevOps
    or Site Reliability Engineer may use this command to gather information about
    recent events in a specific namespace and context to identify any issues causing
    the CrashLoopBackoff events.

    2. Investigating container image pulling issues: If there are issues related to
    pulling container images, the engineer can use this command to deploy a new pod
    using Skopeo to inspect the problematic container image and its available tags.

    3. Checking for the existence of a specific container image: The engineer may
    use this command to verify the existence of a specific container image, and if
    it does not exist, list the available tags for that image.

    4. Maintenance activities: This command could be used as part of regular maintenance
    activities to ensure the availability and health of container images and their
    related pods.

    5. Resolving deployment issues: In the event of deployment issues, the engineer
    may use this command to gather information about the container images being used
    and take action to ensure successful deployment.'
- command: 'NAMESPACE=${NAMESPACE}; POD_NAME="skopeo-pod"; CONTEXT="${CONTEXT}"; events=$(kubectl
    get events -n $NAMESPACE --context=$CONTEXT -o json | jq --arg timestamp "$(date
    -u -v -5M +"%Y-%m-%dT%H:%M:%SZ" 2>/dev/null || date -u -d "-5 minutes" +"%Y-%m-%dT%H:%M:%SZ")"
    ''.items[] | select(.lastTimestamp > $timestamp)''); if [[ ! -z "\${events-unset}"
    ]]; then image_pull_backoff_events=$(echo "$events" | jq -s ''[.[] | select(.reason
    == "BackOff") | .message] | .[]''); else echo "No events found in the last 5 minutes";
    exit; fi; if [[ $image_pull_backoff_events =~ "Back-off pulling image" ]]; then
    echo "Running Skopeo Pod"; kubectl run $POD_NAME --restart=Never -n $NAMESPACE
    --context=$CONTEXT --image=quay.io/containers/skopeo:latest --command -- sleep
    infinity && echo "Waiting for the $POD_NAME to be running..." && kubectl wait
    --for=condition=Ready pod/$POD_NAME -n $NAMESPACE --context=$CONTEXT; else echo
    "No image pull backoff events found"; exit; fi; while IFS= read -r event; do echo
    "Found BackOff with message: $event"; echo "Checking if we can reach the image
    with skopeo and what tags exist"; container_image_path_tag=$(echo "$event" | cut
    -d'' '' -f4 | tr -d ''"'' | tr -d ''\''); container_image_path="\${container_image_path_tag%:*}";
    container_image_tag="\${container_image_path_tag#*:}"; if [ -z "$container_image_path"
    ] || [ -z "$container_image_tag" ]; then continue; fi; skopeo_output=$(kubectl
    exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo inspect docker://$container_image_path:$container_image_tag);
    skopeo_exit_code=$?; if [ $skopeo_exit_code -eq 0 ]; then echo "Container image
    ''$container_image_path:$container_image_tag'' exists."; else echo "Container
    image ''$container_image_path:$container_image_tag'' does not exist."; echo "Available
    tags for ''$container_image_path'':"; available_tags=$(kubectl exec $POD_NAME
    -n $NAMESPACE --context=$CONTEXT -- skopeo list-tags docker://$container_image_path
    ); echo "$available_tags"; fi; done <<<"$image_pull_backoff_events" && echo "Deleting
    Skopeo pod" && kubectl delete pod $POD_NAME -n $NAMESPACE --context=$CONTEXT &&
    echo "Done"'
  doc_links: '

    - [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/){:target="_blank"}

    - [namespace and context in kubernetes](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#context){:target="_blank"}

    - [Shell scripting documentation](https://www.tutorialspoint.com/unix/unix-shell-functions.htm){:target="_blank"}

    - [Skopeo tool documentation](https://github.com/containers/skopeo){:target="_blank"}'
  explanation: This command is a shell script that uses kubectl to gather information
    about recent events in a specific namespace and context. It then evaluates whether
    there have been any "BackOff" events related to pulling container images, and
    if so, it deploys a new pod using the Skopeo tool to inspect the problematic container
    image and its available tags. If the image exists, it prints a message, otherwise
    it lists available tags and finally deletes the Skopeo pod.
  multi_line_details: "\n# Set the NAMESPACE variable to the environment variable\
    \ of the same name\nNAMESPACE=${NAMESPACE}\n\n# Set the POD_NAME variable to \"\
    skopeo-pod\"\nPOD_NAME=\"skopeo-pod\"\n\n# Set the CONTEXT variable to the environment\
    \ variable of the same name\nCONTEXT=\"${CONTEXT}\"\n\n# Get the events from the\
    \ Kubernetes cluster and filter them based on timestamp\nevents=$(kubectl get\
    \ events -n $NAMESPACE --context=$CONTEXT -o json | jq --arg timestamp \"$(date\
    \ -u -v -5M +\"%Y-%m-%dT%H:%M:%SZ\" 2>/dev/null || date -u -d \"-5 minutes\" +\"\
    %Y-%m-%dT%H:%M:%SZ\")\" '.items[] | select(.lastTimestamp > $timestamp)')\n\n\
    # If there are no events available, exit with a message\nif [[ ! -z \"${events-unset}\"\
    \ ]]; then\n  image_pull_backoff_events=$(echo \"$events\" | jq -s '[.[] | select(.reason\
    \ == \"BackOff\") | .message] | .[]')\nelse\n  echo \"No events found in the last\
    \ 5 minutes\"\n  exit\nfi\n\n# Check if the image pull backoff event contains\
    \ the specific message\nif [[ $image_pull_backoff_events =~ \"Back-off pulling\
    \ image\" ]]; then\n  # Run Skopeo Pod with the specified configuration\n  echo\
    \ \"Running Skopeo Pod\"\n  kubectl run $POD_NAME --restart=Never -n $NAMESPACE\
    \ --context=$CONTEXT --image=quay.io/containers/skopeo:latest --command -- sleep\
    \ infinity && echo \"Waiting for the $POD_NAME to be running...\" && kubectl wait\
    \ --for=condition=Ready pod/$POD_NAME -n $NAMESPACE --context=$CONTEXT\nelse\n\
    \  echo \"No image pull backoff events found\"\n  exit\nfi\n\n# Process each image\
    \ pull backoff event by checking for its availability and retrieving available\
    \ tags using Skopeo\nwhile IFS= read -r event; do\n  echo \"Found BackOff with\
    \ message: $event\"\n  echo \"Checking if we can reach the image with skopeo and\
    \ what tags exist\"\n  container_image_path_tag=$(echo \"$event\" | cut -d' '\
    \ -f4 | tr -d '\"' | tr -d '\\')\n  container_image_path=\"${container_image_path_tag%:*}\"\
    \n  container_image_tag=\"${container_image_path_tag#*:}\"\n  if [ -z \"$container_image_path\"\
    \ ] || [ -z \"$container_image_tag\" ]; then\n    continue\n  fi\n  skopeo_output=$(kubectl\
    \ exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo inspect docker://$container_image_path:$container_image_tag)\n\
    \  skopeo_exit_code=$?\n  if [ $skopeo_exit_code -eq 0 ]; then\n    echo \"Container\
    \ image '$container_image_path:$container_image_tag' exists.\"\n  else\n    echo\
    \ \"Container image '$container_image_path:$container_image_tag' does not exist.\"\
    \n    echo \"Available tags for '$container_image_path':\"\n    available_tags=$(kubectl\
    \ exec $POD_NAME -n $NAMESPACE --context=$CONTEXT -- skopeo list-tags docker://$container_image_path\
    \ )\n    echo \"$available_tags\"\n  fi\ndone <<<\"$image_pull_backoff_events\"\
    \n\n# Delete the Skopeo pod after processing all events\necho \"Deleting Skopeo\
    \ pod\"\nkubectl delete pod $POD_NAME -n $NAMESPACE --context=$CONTEXT\necho \"\
    Done\"\n"
  name: list_imagepullbackoff_events_and_test_path_and_tags_for_namespace_namespace
  when_is_it_useful: '1. Troubleshooting Kubernetes CrashLoopBackoff events: The DevOps
    or Site Reliability Engineer may use this command to gather information about
    recent events in a specific namespace and context to identify any issues causing
    the CrashLoopBackoff events.

    2. Investigating container image pulling issues: If there are issues related to
    pulling container images, the engineer can use this command to deploy a new pod
    using Skopeo to inspect the problematic container image and its available tags.

    3. Checking for the existence of a specific container image: The engineer may
    use this command to verify the existence of a specific container image, and if
    it does not exist, list the available tags for that image.

    4. Maintenance activities: This command could be used as part of regular maintenance
    activities to ensure the availability and health of container images and their
    related pods.

    5. Resolving deployment issues: In the event of deployment issues, the engineer
    may use this command to gather information about the container images being used
    and take action to ensure successful deployment.'
